{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Prepare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "base_infer_path = '/evaluation/outputs/qwen/Qwen2.5-7B-SFT/math_eval'\n",
    "rl_infer_path = '/evaluation/outputs/qwen/Qwen2.5-7B-DPO/math_eval'\n",
    "model_name = 'qwen'\n",
    "\n",
    "# Datasets to process\n",
    "datasets = ['math', 'gsm8k', 'minerva_math', 'olympiadbench', 'college_math', 'aime24', 'amc23']\n",
    "# datasets = ['math', 'minerva_math']\n",
    "\n",
    "save_path = './datasets/processed/' + model_name + '/'\n",
    "\n",
    "def get_unique_jsonl_file(folder):\n",
    "    files = glob(os.path.join(folder, '*.jsonl'))\n",
    "    if len(files) != 1:\n",
    "        raise ValueError(f\"No unique jsonl file found in {folder}, found: {files}\")\n",
    "    return files[0]\n",
    "\n",
    "def extract_single(item):\n",
    "    # If the item is a list of length 1, return the single element, otherwise return as is\n",
    "    if isinstance(item, list) and len(item) == 1:\n",
    "        return item[0]\n",
    "    return item\n",
    "\n",
    "for dataset in datasets:\n",
    "    base_dataset_dir = os.path.join(base_infer_path, dataset)\n",
    "    rl_dataset_dir = os.path.join(rl_infer_path, dataset)\n",
    "    if not os.path.exists(base_dataset_dir) or not os.path.exists(rl_dataset_dir):\n",
    "        print(f\"Skip {dataset}, because path does not exist\")\n",
    "        continue\n",
    "\n",
    "    base_jsonl = get_unique_jsonl_file(base_dataset_dir)\n",
    "    rl_jsonl = get_unique_jsonl_file(rl_dataset_dir)\n",
    "\n",
    "    # Read base and rl inference results, align by idx\n",
    "    base_data = {}\n",
    "    with open(base_jsonl, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            idx = item.get('idx')\n",
    "            base_data[idx] = item\n",
    "\n",
    "    rl_data = {}\n",
    "    with open(rl_jsonl, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            idx = item.get('idx')\n",
    "            rl_data[idx] = item\n",
    "\n",
    "    # Four categories\n",
    "    all_correct = []\n",
    "    all_wrong = []\n",
    "    base_right_rl_wrong = []\n",
    "    base_wrong_rl_right = []\n",
    "\n",
    "    for idx in base_data:\n",
    "        if idx not in rl_data:\n",
    "            continue\n",
    "        base_item = base_data[idx]\n",
    "        rl_item = rl_data[idx]\n",
    "\n",
    "        # Extract score\n",
    "        base_score = extract_single(base_item.get('score'))\n",
    "        rl_score = extract_single(rl_item.get('score'))\n",
    "\n",
    "        # Only keep true/false text\n",
    "        base_score_bool = str(base_score).lower() == 'true'\n",
    "        rl_score_bool = str(rl_score).lower() == 'true'\n",
    "\n",
    "        # Assemble output fields\n",
    "        out_item = {\n",
    "            \"idx\": extract_single(idx),\n",
    "            \"question\": extract_single(base_item.get(\"question\")),\n",
    "            \"gt\": extract_single(base_item.get(\"gt\")),\n",
    "            \"solution\": extract_single(base_item.get(\"solution\")),\n",
    "            \"base_prompt_tokens\": extract_single(base_item.get(\"prompt_token_ids\")),\n",
    "            \"base_answer_tokens\": extract_single(base_item.get(\"generate_token_ids\")),\n",
    "            \"base_answer_tokens_range\": [len(extract_single(base_item.get(\"prompt_token_ids\"))), len(extract_single(base_item.get(\"prompt_token_ids\"))) + len(extract_single(base_item.get(\"generate_token_ids\")))],\n",
    "            \"base_pred\": extract_single(base_item.get(\"pred\")),\n",
    "            \"base_score\": base_score,\n",
    "            \"rl_prompt_tokens\": extract_single(rl_item.get(\"prompt_token_ids\")),\n",
    "            \"rl_answer_tokens\": extract_single(rl_item.get(\"generate_token_ids\")),\n",
    "            \"rl_answer_tokens_range\": [len(extract_single(rl_item.get(\"prompt_token_ids\"))), len(extract_single(rl_item.get(\"prompt_token_ids\"))) + len(extract_single(rl_item.get(\"generate_token_ids\")))],\n",
    "            \"rl_pred\": extract_single(rl_item.get(\"pred\")),\n",
    "            \"rl_score\": rl_score\n",
    "        }\n",
    "\n",
    "        if base_score_bool and rl_score_bool:\n",
    "            all_correct.append(out_item)\n",
    "        elif (not base_score_bool) and (not rl_score_bool):\n",
    "            all_wrong.append(out_item)\n",
    "        elif base_score_bool and (not rl_score_bool):\n",
    "            base_right_rl_wrong.append(out_item)\n",
    "        elif (not base_score_bool) and rl_score_bool:\n",
    "            base_wrong_rl_right.append(out_item)\n",
    "\n",
    "    # Save\n",
    "    save_dir = os.path.join(save_path, dataset)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(save_dir, 'all_correct.jsonl'), 'w', encoding='utf-8') as f:\n",
    "        for item in all_correct:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    with open(os.path.join(save_dir, 'all_wrong.jsonl'), 'w', encoding='utf-8') as f:\n",
    "        for item in all_wrong:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    with open(os.path.join(save_dir, 'base_right_rl_wrong.jsonl'), 'w', encoding='utf-8') as f:\n",
    "        for item in base_right_rl_wrong:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    with open(os.path.join(save_dir, 'base_wrong_rl_right.jsonl'), 'w', encoding='utf-8') as f:\n",
    "        for item in base_wrong_rl_right:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"{dataset} processed, saved in {save_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
